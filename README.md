# ğŸ¤– Chatbot WebGPU - Assistant IA Local avec RAG

Un chatbot intelligent fonctionnant **100% en local** dans le navigateur, propulsÃ© par WebGPU et dotÃ© d'un systÃ¨me RAG (Retrieval-Augmented Generation) avancÃ© pour exploiter vos propres bases de connaissances.

## ğŸŒ DÃ©mo en Ligne

**ğŸ‘‰ [Essayer la dÃ©mo maintenant](https://chat-webgpu.onrender.com/) ğŸ‘ˆ**

Testez directement l'application sans installation !

![Version](https://img.shields.io/badge/version-0.1.0-blue)
![Next.js](https://img.shields.io/badge/Next.js-16.0.1-black)
![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue)
![WebLLM](https://img.shields.io/badge/WebLLM-0.2.79-green)

## âœ¨ FonctionnalitÃ©s Principales

### ğŸ§  **Intelligence Artificielle Locale**
- **ModÃ¨les IA** s'exÃ©cutant directement dans le navigateur via WebGPU
- **Aucun serveur externe** - ConfidentialitÃ© totale
- **4 modÃ¨les optimisÃ©s** de 1B Ã  8B paramÃ¨tres
- **Streaming en temps rÃ©el** des rÃ©ponses

### ğŸ’¾ **SystÃ¨me de Conversations**
- **Gestion multi-conversations** avec historique persistant
- **Renommage Ã  la volÃ©e** en cliquant sur le titre
- **Stockage local** via IndexedDB
- **Recherche** dans l'historique des conversations

### ğŸ“š **RAG (Retrieval-Augmented Generation)**
- **Bases de connaissances personnalisÃ©es** 
- **Import multi-sources** : fichiers TXT, Wikipedia
- **Embeddings locaux** avec Transformers.js
- **Recherche sÃ©mantique** intelligent
- **Affichage des sources** utilisÃ©es dans les rÃ©ponses

### ğŸŒ **Import de Connaissances**
- **Wikipedia franÃ§ais** : Recherche et import d'articles complets
- **Fichiers texte** : Support .txt avec validation
- **Chunking intelligent** avec options configurables
- **MÃ©tadonnÃ©es enrichies** (taille, nombre de chunks, date)

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### **Frontend & Framework**
- **Next.js 16.0.1** - Framework React avec App Router
- **React 19.2.0** - Interface utilisateur rÃ©active
- **TypeScript 5.x** - Typage statique
- **Tailwind CSS 4.x** - Styling avec mode sombre
- **React Compiler** - Optimisations automatiques

### **Intelligence Artificielle**
- **WebLLM (@mlc-ai/web-llm)** - ModÃ¨les LLM dans le navigateur
- **Transformers.js (@xenova/transformers)** - ModÃ¨les d'embedding
- **WebGPU** - AccÃ©lÃ©ration GPU native
- **SharedArrayBuffer** - Performance optimisÃ©e

### **Stockage & DonnÃ©es**
- **IndexedDB** - Base de donnÃ©es locale du navigateur
- **JSON** - SÃ©rialisation des donnÃ©es
- **Blob Storage** - Gestion des gros volumes

---

## ğŸ¤– ModÃ¨les d'IA Disponibles

### **ModÃ¨les de Chat (WebLLM)**

| ModÃ¨le | Taille | RAM Min | Vitesse | QualitÃ© | Mobile | Description |
|--------|--------|---------|---------|---------|---------|-------------|
| **Llama 3.2 1B** | ~2.5GB | 4GB | Rapide | Bon | âœ… | Petit Llama, adaptÃ© au RAG trÃ¨s simple ou local search |
| **Phi-3.5 3.8B** | ~5.5GB | 8GB | Rapide | Excellent | âŒ | Un des meilleurs modÃ¨les <4B pour du vrai RAG sÃ©rieux |
| **Qwen 2.5 3B** | ~6GB | 8GB | TrÃ¨s rapide | Excellent | âŒ | TrÃ¨s bon grounding, excellent sur RAG multi-chunks |
| **Llama 3.1 8B** | ~12GB | 12GB | Lent | Excellent | âŒ | Pour du RAG avancÃ© avec contexte large. TrÃ¨s fiable |

### **ModÃ¨les d'Embedding (Transformers.js)**

| ModÃ¨le | Taille | Dimensions | Langues | Usage |
|--------|--------|------------|---------|-------|
| **all-MiniLM-L6-v2** | ~90MB | 384D | EN/FR | Rapide, qualitÃ© correcte |
| **all-MiniLM-L12-v2** | ~120MB | 384D | EN/FR | Plus lent, meilleure qualitÃ© |

---

## ğŸ—ï¸ Architecture du SystÃ¨me

### **Structure des Composants**

```
src/
â”œâ”€â”€ app/                          # Next.js App Router
â”‚   â”œâ”€â”€ page.tsx                 # Interface principale
â”‚   â”œâ”€â”€ layout.tsx               # Layout global
â”‚   â””â”€â”€ api/extract-text/        # API extraction de texte
â”œâ”€â”€ components/                   # Composants React
â”‚   â”œâ”€â”€ ConversationSidebar.tsx  # Gestion conversations
â”‚   â”œâ”€â”€ KnowledgeManager.tsx     # Gestion des bases de donnÃ©es
â”‚   â”œâ”€â”€ ModelSelector.tsx        # SÃ©lection de modÃ¨le
â”‚   â”œâ”€â”€ RAGControls.tsx         # ContrÃ´les RAG
â”‚   â””â”€â”€ WikipediaImporter.tsx   # Import Wikipedia
â”œâ”€â”€ hooks/                       # React Hooks
â”‚   â”œâ”€â”€ useWebLLM.ts            # Gestion modÃ¨les WebLLM
â”‚   â”œâ”€â”€ useConversations.ts     # Gestion conversations
â”‚   â””â”€â”€ useKnowledgeBase.ts     # Gestion bases de connaissances
â”œâ”€â”€ lib/                         # BibliothÃ¨ques utilitaires
â”‚   â”œâ”€â”€ embedding.ts            # Gestion embeddings
â”‚   â”œâ”€â”€ rag.ts                  # SystÃ¨me RAG
â”‚   â”œâ”€â”€ documentProcessor.ts    # Traitement documents
â”‚   â”œâ”€â”€ conversationStorage.ts  # Stockage conversations
â”‚   â”œâ”€â”€ indexedDB.ts           # Base de donnÃ©es locale
â”‚   â””â”€â”€ wikipediaService.ts    # Service Wikipedia
â””â”€â”€ types/                      # DÃ©finitions TypeScript
    â”œâ”€â”€ models.ts              # Types modÃ¨les IA
    â””â”€â”€ conversation.ts        # Types conversations
```

---

## ğŸ“Š Fonctionnement du RAG

### **Pipeline de Traitement des Documents**

1. **ğŸ“¥ Import de Sources**
   - Upload de fichiers .txt
   - Recherche et import d'articles Wikipedia
   - Validation du contenu

2. **âœ‚ï¸ Chunking Intelligent**
   ```typescript
   interface ChunkingOptions {
     chunkSize: 512,        // Taille des chunks
     chunkOverlap: 50,      // Chevauchement
     splitOn: 'sentence',   // DÃ©coupage par phrase
     minChunkSize: 100,     // Taille minimale
     maxChunkSize: 1000     // Taille maximale
   }
   ```

3. **ğŸ§® GÃ©nÃ©ration d'Embeddings**
   - ModÃ¨le all-MiniLM-L6-v2 (384 dimensions)
   - Calcul local avec Transformers.js
   - Stockage dans IndexedDB

4. **ğŸ” Recherche SÃ©mantique**
   ```typescript
   // Recherche par similaritÃ© cosinus
   const results = await ragManager.search(
     query,
     knowledgeBase,
     {
       maxResults: 5,
       threshold: 0.7
     }
   );
   ```

5. **ğŸ’¬ Augmentation du Context**
   - Injection des chunks pertinents
   - Enrichissement du prompt utilisateur
   - GÃ©nÃ©ration de rÃ©ponse contextualisÃ©e

### **Exemple de Workflow RAG**

```typescript
// 1. Question utilisateur
const userQuery = "Comment fonctionne l'intelligence artificielle ?"

// 2. Recherche dans la base de connaissances
const relevantChunks = await ragManager.search(userQuery, knowledgeBase)

// 3. Construction du context enrichi
const contextualPrompt = `
Contexte : ${relevantChunks.map(c => c.content).join('\n')}

Question : ${userQuery}
`

// 4. GÃ©nÃ©ration avec le modÃ¨le LLM
const response = await engine.generateResponse(contextualPrompt)
```

---

## âš™ï¸ Configuration & Optimisations

### **Next.js Configuration**

```typescript
// next.config.ts
const nextConfig = {
  // Support WebAssembly pour WebLLM
  webpack: (config) => ({
    ...config,
    experiments: {
      asyncWebAssembly: true,
      layers: true
    }
  }),
  
  // Headers COOP/COEP pour SharedArrayBuffer
  headers: async () => [{
    source: '/:path*',
    headers: [
      { key: 'Cross-Origin-Embedder-Policy', value: 'require-corp' },
      { key: 'Cross-Origin-Opener-Policy', value: 'same-origin' }
    ]
  }]
}
```

### **Optimisations Performance**

- **Quantization Q4** pour les modÃ¨les LLM (4x plus petit)
- **Chunking adaptatif** selon la taille du document
- **Cache des embeddings** pour Ã©viter les recalculs
- **Lazy loading** des composants lourds
- **IndexedDB avec index** pour les recherches rapides

---

## ğŸš€ Installation & Utilisation

### **PrÃ©requis**
- **Navigateur compatible WebGPU** (Chrome/Edge 113+, Firefox Nightly)
- **RAM minimum** : 4GB pour les petits modÃ¨les, 8GB recommandÃ©
- **Connexion internet** uniquement pour le tÃ©lÃ©chargement initial des modÃ¨les

### **Installation**

```bash
# Cloner le repository
git clone <repository-url>
cd chatbot-webgpu

# Installer les dÃ©pendances
npm install

# Lancer en dÃ©veloppement
npm run dev

# Ou build pour production
npm run build
npm start
```

### **PremiÃ¨re Utilisation**

1. **ğŸ”§ SÃ©lectionner un modÃ¨le IA**
   - Cliquer sur "SÃ©lectionner un modÃ¨le" dans le header
   - Choisir selon votre RAM disponible
   - Attendre le tÃ©lÃ©chargement (une seule fois)

2. **ğŸ“š CrÃ©er une base de connaissances (optionnel)**
   - Aller dans l'onglet "Knowledge"
   - Charger un modÃ¨le d'embedding
   - Importer des fichiers ou articles Wikipedia
   - Attendre le traitement

3. **ğŸ’¬ Commencer Ã  chatter**
   - Revenir Ã  l'onglet "Chat"
   - Activer le RAG si vous avez des bases de connaissances
   - Poser vos questions !

---

## ğŸ¯ Cas d'Usage

### **ğŸ“– Assistant Documentation**
- Import de documentation technique
- RÃ©ponses prÃ©cises basÃ©es sur vos docs
- Sources citÃ©es automatiquement

### **ğŸ“ Tuteur PersonnalisÃ©**
- Import de cours et supports
- Explications contextualisÃ©es
- Apprentissage adaptÃ© Ã  vos contenus

### **ğŸ” Analyseur de Corpus**
- Traitement de gros volumes de texte
- Recherche sÃ©mantique avancÃ©e
- SynthÃ¨ses et rÃ©sumÃ©s intelligents

### **ğŸŒ Explorateur Wikipedia**
- Import d'articles complets
- Base de connaissances thÃ©matique
- Questions-rÃ©ponses factuelles

---

## ğŸ“ˆ MÃ©triques & Performances

### **Tailles de ModÃ¨les**
- **Llama 3.2 1B** : ~2.5GB (compact et efficace)
- **Phi-3.5 3.8B** : ~5.5GB (recommandÃ© pour RAG)
- **Qwen 2.5 3B** : ~6GB (excellente qualitÃ©/vitesse)
- **Llama 3.1 8B** : ~12GB (RAG avancÃ©)
- **Embeddings** : ~90-120MB

### **Performance Type**
- **PremiÃ¨re gÃ©nÃ©ration** : 2-5 secondes (selon modÃ¨le)
- **GÃ©nÃ©rations suivantes** : 0.5-2 secondes
- **Recherche RAG** : <100ms pour 1000 chunks
- **Import Wikipedia** : 1-5 secondes par article

### **Limites Techniques**
- **Context window** : 2048-4096 tokens selon le modÃ¨le
- **Chunks par requÃªte** : 3-5 recommandÃ©
- **Taille max document** : 10MB par fichier
- **Storage browser** : LimitÃ© par IndexedDB (~50-100GB)

---

## ğŸ”§ API & ExtensibilitÃ©

### **Services Principaux**

```typescript
// WebLLM Engine
const { initModel, generateResponse } = useWebLLM()

// Knowledge Management
const { createKnowledgeBase, deleteKnowledgeBase } = useKnowledgeBase()

// RAG System
const results = await ragManager.search(query, knowledgeBase)

// Wikipedia Service
const article = await wikipediaService.getArticleContent(title)
```

### **Types TypeScript**

```typescript
interface KnowledgeBase {
  id: string
  name: string
  description: string
  chunks: DocumentChunk[]
  totalDocuments: number
  totalChunks: number
  sizeBytes: number
  createdAt: Date
}

interface DocumentChunk {
  id: string
  content: string
  embedding: number[]
  metadata: {
    sourceName: string
    chunkIndex: number
    startChar: number
    endChar: number
  }
}
```

---

## ğŸ›¡ï¸ SÃ©curitÃ© & ConfidentialitÃ©

### **ğŸ”’ 100% Local**
- **Aucune donnÃ©e envoyÃ©e** vers des serveurs externes
- **ModÃ¨les s'exÃ©cutent** entiÃ¨rement dans le navigateur
- **Stockage local** via IndexedDB sÃ©curisÃ©

### **ğŸ› ï¸ ContrÃ´le Total**
- **Code source ouvert** et auditable
- **Pas de tÃ©lÃ©mÃ©trie** ou tracking
- **ModÃ¨les offline** aprÃ¨s tÃ©lÃ©chargement initial

### **âš¡ Performance WebGPU**
- **AccÃ©lÃ©ration GPU native** pour l'infÃ©rence
- **Optimisations WASM** pour les calculs
- **Memory mapping** efficace

---

## ğŸ”® Roadmap & Extensions Futures

### **ğŸ¯ FonctionnalitÃ©s PrÃ©vues**
- [ ] Support formats additionnels (PDF, DOCX, EPUB)
- [ ] Import depuis URLs web avec scraping
- [ ] Transcription vidÃ©os YouTube (Whisper local)
- [ ] ModÃ¨les d'embedding multilingues
- [ ] Export/Import des bases de connaissances
- [ ] API REST pour intÃ©grations externes

### **ğŸš€ Optimisations Techniques**
- [ ] Quantization INT8 pour modÃ¨les plus lÃ©gers
- [ ] Worker threads pour embeddings
- [ ] Progressive loading des gros modÃ¨les
- [ ] Compression avancÃ©e des chunks
- [ ] Cache intelligent multi-sessions

---

## ğŸ“‹ DÃ©pendances

### **Production**
```json
{
  "@mlc-ai/web-llm": "^0.2.79",      // Moteur LLM WebGPU
  "@xenova/transformers": "^2.17.2",  // Embeddings & NLP
  "next": "16.0.1",                   // Framework React
  "react": "19.2.0",                  // UI Library
  "formidable": "^3.5.4",            // Upload de fichiers
  "lucide-react": "^0.555.0",        // IcÃ´nes
  "mammoth": "^1.11.0",              // Parsing DOCX
  "pdf-parse": "^2.4.5"              // Parsing PDF
}
```

### **DÃ©veloppement**
```json
{
  "typescript": "^5",                 // Typage statique
  "tailwindcss": "^4",               // CSS Framework
  "eslint": "^9",                    // Linting
  "babel-plugin-react-compiler": "1.0.0" // Optimisations React
}
```

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- ğŸ› Signaler des bugs
- ğŸ’¡ Proposer de nouvelles fonctionnalitÃ©s  
- ğŸ”§ Soumettre des Pull Requests
- ğŸ“– AmÃ©liorer la documentation

---

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- ğŸ“§ **Issues GitHub** : ProblÃ¨mes techniques
- ğŸ’¬ **Discussions** : Questions gÃ©nÃ©rales
- ğŸ“š **Wiki** : Documentation complÃ¨te

---

**DÃ©veloppÃ© avec â¤ï¸ pour une IA accessible et respectueuse de la vie privÃ©e**
